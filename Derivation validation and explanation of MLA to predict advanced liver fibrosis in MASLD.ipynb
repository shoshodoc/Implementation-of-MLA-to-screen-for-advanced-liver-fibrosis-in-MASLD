{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99878403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the clean dataset:\n",
    "df = pd.read_csv(r\"C:\\Users\\DELL\\Desktop\\ML training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a515044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the modeling pipeline\n",
    "###data preprocessing with pipline.\n",
    "### we have one nominal feature- gender and two ordinal features- HTN and DM or IFG\n",
    "### numerical features will be mean-centered and scaled.\n",
    "### lastly, iterative imputer will be applied.\n",
    "\n",
    "numerical_ix = df_clean.select_dtypes(include=['float64']).columns\n",
    "ordinal_ix = df_clean.select_dtypes(include=['int32']).columns\n",
    "nominal_ix = df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,StandardScaler, PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "numeric_transformer = Pipeline(steps=[('standard&scale',StandardScaler()),(\"yeo-johnson\", PowerTransformer(method='yeo-johnson', standardize=False))])\n",
    "ordinal_transformer = Pipeline(steps=[('ordinal',  OrdinalEncoder())])\n",
    "nominal_transformer = Pipeline(steps=[('nominal', OneHotEncoder())])\n",
    "\n",
    "col_transform = ColumnTransformer(transformers=[(\"numeric\", numeric_transformer, numerical_ix),\n",
    "        (\"nominal\", nominal_transformer, nominal_ix),(\"ordinal\",ordinal_transformer,ordinal_ix)],\n",
    "                                  verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69733f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the LASSO logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty=\"l1\",solver='liblinear',max_iter=1000)\n",
    "\n",
    "pipe_LR = Pipeline(steps = [('transormations', col_transform), ('imputation', imputer),\n",
    "                         ('classifier',classifier)])\n",
    "\n",
    "#setting the dictionary for penalization tunning parameter\n",
    "para_grid_lasso = {\"classifier__C\":np.logspace(-4,2,50)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting LASSO Logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tunning_c = GridSearchCV(pipe_LR, para_grid_lasso,scoring='roc_auc',return_train_score=True)\n",
    "\n",
    "tunning_c.fit(df_clean,y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the SVM \n",
    "rom sklearn import svm\n",
    "classifier_SVM = svm.SVC(tol=0.01,probability=True)\n",
    "\n",
    "SVM_pipe = Pipeline(steps = [('transormations', col_transform), ('imputation', imputer),\n",
    "                         ('classifier',classifier_SVM)])\n",
    "\n",
    "#setting the dictionary for tunning parameters\n",
    "SVM_grid = SVM_grid = [{'classifier__kernel':['linear'],'classifier__C': np.logspace(-4,1,10)},\n",
    "              {'classifier__kernel':['poly'],'classifier__C': np.logspace(-4,1,10),'classifier__degree':np.arange(2,4)}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting SVM\n",
    "SVM_tune = GridSearchCV(SVM_pipe, param_grid=SVM_grid, scoring='roc_auc',return_train_score=True,n_jobs=-1,verbose=3)\n",
    "\n",
    "SVM_tune.fit(df_clean,y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4862668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_classifier = RandomForestClassifier(n_estimators=500)\n",
    "pipe_RF = Pipeline(steps = [('transormations', col_transform),('imputation', imputer),('classifier',RF_classifier)])\n",
    "\n",
    "#setting the dictionary for tunning parameters\n",
    "RF_grid = RF_grid = [{'classifier__max_depth':[4,5,6],'classifier__min_samples_split': [18,20,25],\n",
    "                      'classifier__max_features': [0.1,0.2],'classifier__max_samples':[0.5,0.6],'classifier__min_impurity_decrease':[0.001,0.005,0.01]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting random forest\n",
    "RF_tune_2 = GridSearchCV(pipe_RF, param_grid=RF_grid, scoring='roc_auc',return_train_score=True,n_jobs=-1,verbose=3)\n",
    "\n",
    "RF_tune_2.fit(df_clean,y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipe_nn= Pipeline(steps = [('transormations', col_transform),('imputation', imputer),('classifier',deep_layer_nn)])\n",
    "\n",
    "#setting the network architeture\n",
    "deep_layer_nn = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(5,3),max_iter=15000,random_state=1)\n",
    "\n",
    "#setting the dictionary for tunning parameter L2\n",
    "nn_grid = [{'classifier__alpha':[8,9,10,11,12,13,14]}] \n",
    "\n",
    "nn_tune = GridSearchCV(pipe_nn, param_grid=nn_grid, cv=5, scoring='roc_auc',return_train_score=True,n_jobs=-1,verbose=3)\n",
    "\n",
    "nn_tune.fit(df_clean,y_tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting ANN\n",
    "nn_tune = GridSearchCV(pipe_nn, param_grid=nn_grid, cv=5, scoring='roc_auc',return_train_score=True,n_jobs=-1,verbose=3)\n",
    "\n",
    "nn_tune.fit(df_clean,y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting the XGBoost:\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xg_classifier = XGBClassifier(objective='binary:logistic',eval_metric='auc')\n",
    "\n",
    "pipe_XG = Pipeline(steps = [('transormations', col_transform),('imputation', imputer),('classifier',xg_classifier)])\n",
    "\n",
    "#setting the dictionary for tunning parameters\n",
    "xg_grid = {'classifier__max_depth':[2,3,4],'classifier__n_estimators':[700,750,800,850,950],\n",
    "           'classifier__eta':[0.15,0.1,0.01],\n",
    "           'classifier__colsample_bytree':[0.4,0.5],'classifier__subsample':[0.4,0.5],'classifier__alpha':[7],\n",
    "           'classifier__lambda':[7]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting XGBoost\n",
    "xg_tune =  GridSearchCV(pipe_XG, param_grid=xg_grid, scoring='roc_auc',return_train_score=True,n_jobs=-1,verbose=3)\n",
    "\n",
    "xg_tune.fit(df_clean,y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display ROC curves for each model\n",
    "#In the same loop take the concatenate dataset of validation folds with FIB-4 NFS and XGBoost predictions to calculate sample size\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve,det_curve\n",
    "from sklearn.model_selection import StratifiedKFold,\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# initializing XGBoost with the best parameters \n",
    "xg_classifier = XGBClassifier(alpha= xg_tune.best_params_['classifier__alpha'],colsample_bytree= xg_tune.best_params_['classifier__colsample_bytree'],\n",
    "                              eta= xg_tune.best_params_['classifier__eta'],reg_lambda= xg_tune.best_params_['classifier__lambda'],\n",
    "                              max_depth=xg_tune.best_params_['classifier__max_depth'],n_estimators=xg_tune.best_params_['classifier__n_estimators'],\n",
    "                              subsample =xg_tune.best_params_['classifier__subsample'], objective='binary:logistic',eval_metric='auc')\n",
    "\n",
    "pipe_XG = Pipeline(steps = [('transormations', col_transform),('imputation', imputer),('classifier',xg_classifier)])\n",
    "\n",
    "sample_size_cal = list()\n",
    "target_sample_cal = list()\n",
    "FIB = list()\n",
    "NFS = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "skf.get_n_splits(df_clean, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "fpr=[]\n",
    "tpr=[]\n",
    "thresholds=[]\n",
    "threshold=np.zeros(5)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(df_clean, y)):\n",
    "    x_train, x_test = df_clean.iloc[train_index,:], df_clean.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    label_encoder = label_encoder.fit(y_train)\n",
    "    label_encoded_y_train = label_encoder.transform(y_train)\n",
    "    label_encoded_y_test = label_encoder.transform(y_test)\n",
    "    pipe_XG.fit(x_train,label_encoded_y_train)\n",
    "    sample_size_cal.append(pipe_XG.predict_proba(x_test)[:,1])\n",
    "    FIB.append(df_clean.iloc[test_index,:].apply(lambda x: (x['age'] * x['AST'])/(x['PLT']*x['ALT']**0.5), axis=1))\n",
    "    NFS.append(df_clean.iloc[test_index,:].apply(lambda x: (-(1.675) + 0.037 *x['age'] + 0.094 * x['BMI'] + 1.13 * x['DM or IFG'] + 0.99 *x['AST']/x['ALT'] - 0.013 *x['PLT'] - 0.66 *x['albumin']), axis=1))\n",
    "    target_sample_cal.append(label_encoded_y_test)\n",
    "    viz = RocCurveDisplay.from_estimator(pipe_XG,x_test,label_encoded_y_test,name=f\"ROC fold {i+1}\",alpha=0.5,lw=3,ax=ax)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    #title=\"ROC curves for 5 folds\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the evaluation folds to R for sample size calculation\n",
    "\n",
    "sample_pred=np.concatenate((sample_size_cal[0],sample_size_cal[1],sample_size_cal[2],sample_size_cal[3],sample_size_cal[4]))\n",
    "\n",
    "\n",
    "sample_target=np.concatenate((target_sample_cal[0],target_sample_cal[1],target_sample_cal[2],target_sample_cal[3],target_sample_cal[4]))\n",
    "\n",
    "sample_FIB=np.concatenate((FIB[0],FIB[1],FIB[2],FIB[3],FIB[4]))\n",
    "\n",
    "sample_NFS=np.concatenate((NFS[0],NFS[1],NFS[2],NFS[3],NFS[4]))\n",
    "\n",
    "data_4_export = {'prediction':sample_pred,'label':sample_target,'NFS':sample_NFS,'FIB':sample_FIB}\n",
    "data_4_export = pd.DataFrame(data_4_export)\n",
    "\n",
    "#data_4_export.to_csv('sample size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection error curves of folds and choosing a threshold subjected to resampling for validation\n",
    "from sklearn.metrics import DetCurveDisplay,roc_curve\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fpr=[]\n",
    "tpr=[]\n",
    "thresholds=[]\n",
    "threshold_xg_90=np.zeros(5)\n",
    "sensitivity = np.zeros(5)\n",
    "specificity = np.zeros(5)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(df_clean, pd.Series(y_tt))):\n",
    "    x_train, x_test = df_clean.iloc[train_index,:], df_clean.iloc[test_index,:]\n",
    "    y_train, y_test = pd.Series(y_tt).iloc[train_index], pd.Series(y_tt).iloc[test_index]\n",
    "    fited=pipe_XG.fit(x_train,y_train)\n",
    "    viz =  DetCurveDisplay.from_estimator(fited,x_test,y_test,name=f\"DET fold {i+1}\",alpha=0.5,lw=3,ax=ax)#,pos_label='advanced')\n",
    "    yhat = fited.predict_proba(x_test)\n",
    "    fpr, fnr, thresholds= det_curve(y_test, yhat[:, 1],pos_label=1)\n",
    "    fpr2, tpr, thresholds2 = roc_curve(y_test, yhat[:, 1],pos_label=1)\n",
    "    print(fnr[np.absolute(fnr-0.1).argmin()])\n",
    "    print(fpr[np.absolute(fnr-0.1).argmin()])\n",
    "    print(thresholds[np.absolute(fnr-0.1).argmin()])\n",
    "    print(threshold_xg_90[i])\n",
    "    print(tpr[np.absolute(tpr-0.9).argmin()])\n",
    "    sensitivity[i] = tpr[np.absolute(tpr-0.9).argmin()]\n",
    "    print(1-fpr2[np.absolute(tpr-0.9).argmin()])\n",
    "    specificity[i] = 1-fpr2[np.absolute(tpr-0.9).argmin()]\n",
    "    print(thresholds2[np.absolute(tpr-0.9).argmin()])\n",
    "    threshold_xg_90[i] = thresholds2[np.absolute(tpr-0.9).argmin()]\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    #title=\"DET curves for 5 folds\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final threshold is the mean\n",
    "threshold_xg_90.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa527b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the test data\n",
    "df_test = pd.read_csv(r\"C:\\Users\\DELL\\Desktop\\ML test.csv\")\n",
    "y_test = df_test['Fibrosis']\n",
    "y_testtt = np.zeros(540)\n",
    "y_testtt[y_test=='advanced']=1\n",
    "y_testtt[y_test=='non advanced']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f882013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-way partial dependence-\n",
    "from sklearn.inspection import PartialDependenceDisplay,partial_dependence\n",
    "#choose your feature\n",
    "pdp_HbA1C = partial_dependence(xg_tune.best_estimator_,df_test,features = 'HbA1C',kind=\"average\")\n",
    "\n",
    "#visualize\n",
    "plt.plot(pdp_HbA1C['values'][0], (pdp_HbA1C.average[0]), label = \"HbA1C\")\n",
    "plt.ylabel('partial dependence')\n",
    "plt.legend(loc= \"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f03147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two-way partial dependence\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d  # noqa: F401\n",
    "#choose your features \n",
    "pdp_HbA1C = partial_dependence(xg_tune.best_estimator_,df_test,features = 'HbA1C',kind=\"average\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 25),subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "XX, YY = np.meshgrid(pdp_HbA1C_PLT[\"values\"][0],pdp_HbA1C_PLT[\"values\"][1])\n",
    "Z = pdp_HbA1C_PLT.average[0].T\n",
    "surf = ax.plot_surface(XX, YY, Z, rstride=5, cstride=5, cmap='Greys', edgecolor=\"k\")\n",
    "ax.view_init(elev=20, azim=220)# don't touch 20,220\n",
    "ax.set_ylabel('Platelets (K/microL)',fontsize=30)\n",
    "ax.xaxis.labelpad = 15\n",
    "ax.set_xlabel('HbA1C (%)',fontsize=30)\n",
    "ax.yaxis.labelpad = 17\n",
    "ax.set_zlabel('Probability of advanced fibrosis',fontsize=30)\n",
    "ax.zaxis.labelpad = 17\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()+ax.get_zticklabels()):label.set_fontsize(30)\n",
    "\n",
    "\n",
    "ax.view_init(elev=20, azim=120)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43328446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance derived from training via XGBoost algorithm\n",
    "importance = xg_tune.best_estimator_[2].feature_importances_\n",
    "indices = np.argsort(importance)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.barh(range(len(importance)),importance[indices],color='grey')\n",
    "ax.set_yticks(range(len(importance)))\n",
    "_=ax.set_yticklabels(['Sex: Male','Triglycerides','Sex:Female','Cholesterol','BMI','ALT','Diabetes or IFG','ALP','LDL-c','HDL-c', 'Albumin', 'Hypertension', 'Age', 'Platelets', 'GGT', 'AST', 'HbA1c'])\n",
    "_=ax.set_title('Feature importance via XGboost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orgnaize feature importance via XGBoost on train and test and logistic regression on test\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weights = compute_sample_weight(class_weight='balanced',y=y_testtt)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "premutation_XG = permutation_importance(\n",
    "    xg_tune.best_estimator_[2], xg_tune.best_estimator_[0:2].transform(df_test),y_testtt, n_repeats=100, random_state=22,sample_weight=sample_weights)\n",
    "\n",
    "sorted_importances_idx = premutation_XG.importances_mean.argsort()\n",
    "premutation_XG_df = pd.DataFrame(\n",
    "    premutation_XG.importances[sorted_importances_idx].T,\n",
    "    columns=pipe_XG[:-1].get_feature_names_out()[sorted_importances_idx])\n",
    "\n",
    "premutation_LR = permutation_importance(\n",
    "    tunning_c.best_estimator_[2], tunning_c.best_estimator_[0:2].transform(df_test), y_testtt, n_repeats=100, random_state=2,sample_weight=sample_weights)\n",
    "\n",
    "sorted_importances_idx = premutation_LR.importances_mean.argsort()\n",
    "\n",
    "premutation_LR_df = pd.DataFrame(\n",
    "    premutation_LR.importances[sorted_importances_idx].T,\n",
    "    columns=pipe_LR[:-1].get_feature_names_out()[sorted_importances_idx])\n",
    "\n",
    "from scipy.stats import shapiro,ttest_ind,mannwhitneyu\n",
    "results_dic = {}\n",
    "for column in pipe_XG[:-1].get_feature_names_out():\n",
    "    results_dic[column] = []\n",
    "    if(shapiro(premutation_LR_df[column])[1]<0.05 and shapiro(premutation_XG_df[column])[1]<0.05):\n",
    "        results_dic[column]= [ttest_ind(premutation_LR_df[column], premutation_XG_df[column])[1],\n",
    "                              'T test',premutation_XG_df.columns.get_loc(column),\n",
    "                             premutation_XG_df[column].mean(),\n",
    "                              premutation_LR_df.columns.get_loc(column),premutation_LR_df[column].mean()]\n",
    "    else:\n",
    "        results_dic[column]= [mannwhitneyu(premutation_LR_df[column], premutation_XG_df[column])[1],\n",
    "                              'mann-whitney',premutation_XG_df.columns.get_loc(column),\n",
    "                              premutation_XG_df[column].mean(),\n",
    "                              premutation_LR_df.columns.get_loc(column),premutation_LR_df[column].mean()]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table\n",
    "results_df = pd.DataFrame.from_dict(results_dic, orient='index',\n",
    "                       columns=['p value', 'test', 'XGboost rank','XGboost mean decrease in AUC','LR rank','LR mean decrease in AUC'])\n",
    "training_rank = [((np.where(a==xg_tune.best_estimator_[:-1].get_feature_names_out()[indices]))[0]).item(0) for a in xg_tune.best_estimator_[:-1].get_feature_names_out()]\n",
    "results_df['XG rank on train via impurity'] = training_rank\n",
    "results_df['p value'] = np.round(results_df['p value'],4)\n",
    "results_df['XGboost mean decrease in AUC'] = np.round(results_df['XGboost mean decrease in AUC'],3)\n",
    "results_df['LR mean decrease in AUC'] = np.round(results_df['LR mean decrease in AUC'],3)\n",
    "results_df = results_df[['p value', 'test', 'XGboost rank','XGboost mean decrease in AUC','XG rank on train via impurity','LR rank','LR mean decrease in AUC']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9404a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing feature importance via permutation on test- XGBoost VS logistic regression \n",
    "x = (np.arange(len(results_df.index)))*4.2  # the label locations\n",
    "\n",
    "dic4plot = {'XGBoost':results_df.loc[:,\"XGboost mean decrease in AUC\"].tolist(),'Logistic regression':results_df.loc[:, \"LR mean decrease in AUC\"].tolist()}\n",
    "width = 2  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 15),layout='constrained')\n",
    "\n",
    "for attribute, measurement in dic4plot.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=1,fontsize=18)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Mean decrease in AUC',fontsize=30)\n",
    "ax.set_xlabel('Features',fontsize=30)\n",
    "#ax.set_title('Features by algorithm')\n",
    "ax.set_xticks(x+width/2,['BMI','Platletes','Age','Albumin','ALT','AST','ALP','GGT','HbA1c','HDL-c','LDL-c','Cholesterol','Triglycerides','Female','Male','DM or IFG','Hypertension'],fontsize=25,rotation = 45)\n",
    "ax.tick_params(axis='y', labelsize=25)\n",
    "ax.legend(loc='upper left', ncols=2,fontsize = 30)\n",
    "ax.set_ylim(-0.02, 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19262465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost explainability with shap \n",
    "import shap\n",
    "explainer = shap.TreeExplainer(xg_tune.best_estimator_[2])\n",
    "shap_interaction_values = explainer.shap_interaction_values(xg_tune.best_estimator_[0:2].transform(df_test))\n",
    "mean_shap = np.absolute(shap_interaction_values).mean(0)\n",
    "SHAP_df = pd.DataFrame(mean_shap,index = list(pipe_XG[:-1].get_feature_names_out()), columns = list(pipe_XG[:-1].get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db58555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interaction heat map in terms of shap values\n",
    "SHAP_df_LT = SHAP_df.where(np.tril(np.ones(SHAP_df.shape)).astype(np.bool))\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "im = ax.imshow(SHAP_df_LT.to_numpy(),norm=None,cmap='Greys')\n",
    "\n",
    "ax.set_xticks(np.arange(len(list(pipe_XG[:-1].get_feature_names_out()))), labels=['BMI','Platletes','Age','Albumin','ALT','AST','ALP','GGT','HbA1c','HDL-c','LDL-c','Cholesterol','Triglycerides','Female','Male','DM or IFG','Hypertension']\n",
    ",fontsize=25)\n",
    "ax.set_yticks(np.arange(len(list(pipe_XG[:-1].get_feature_names_out()))), labels=['BMI','Platletes','Age','Albumin','ALT','AST','ALP','GGT','HbA1c','HDL-c','LDL-c','Cholesterol','Triglycerides','Female','Male','DM or IFG','Hypertension']\n",
    ",fontsize=25)\n",
    "\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(list(pipe_XG[:-1].get_feature_names_out()))):\n",
    "    for j in range((len(list(pipe_XG[:-1].get_feature_names_out()))-1),(i-1),-1):\n",
    "        text = ax.text(i, j,np.round(SHAP_df.to_numpy(),2)[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"lightslategrey\",fontsize=20)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b396a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis- identify shap patterns associated with FP\n",
    "from scipy.stats import shapiro,ttest_ind,mannwhitneyu\n",
    "shap_interaction_values_FP = explainer.shap_interaction_values(xg_tune.best_estimator_[0:2].transform(df_test.loc[(y_testtt==0) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)]))\n",
    "shap_interaction_values_TP = explainer.shap_interaction_values(xg_tune.best_estimator_[0:2].transform(df_test.loc[(y_testtt==1) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)]))\n",
    "mean_shap_FP = pd.DataFrame(shap_interaction_values_FP.mean(0),index = list(pipe_XG[:-1].get_feature_names_out()), columns = list(pipe_XG[:-1].get_feature_names_out()))\n",
    "mean_shap_TP = pd.DataFrame(shap_interaction_values_TP.mean(0),index = list(pipe_XG[:-1].get_feature_names_out()), columns = list(pipe_XG[:-1].get_feature_names_out()))\n",
    "# T test-\n",
    "hypothesis_matirix = np.ones(SHAP_df.shape)\n",
    "\n",
    "for i in range(len(list(pipe_XG[:-1].get_feature_names_out()))):\n",
    "    for j in range(len(list(pipe_XG[:-1].get_feature_names_out()))):\n",
    "        hypothesis_matirix[j,i] = ttest_ind(shap_interaction_values_FP[:,j,i],shap_interaction_values_TP[:,j,i])[1]\n",
    "\n",
    "SHAP_hypothesis_results = pd.DataFrame(hypothesis_matirix,index = list(pipe_XG[:-1].get_feature_names_out()), columns = list(pipe_XG[:-1].get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the hypothesis testing for shap of FP vs TN to identify patterns associated with FP\n",
    "SHAP_hypothesis_LT = SHAP_df.where(np.tril(np.ones(SHAP_df.shape)).astype(np.bool))\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "im = ax.imshow(SHAP_hypothesis_LT.to_numpy(),norm=None,cmap='OrRd')\n",
    "ax.set_xticks(np.arange(len(list(pipe_XG[:-1].get_feature_names_out()))), labels=list(pipe_XG[:-1].get_feature_names_out()),fontsize=15)\n",
    "ax.set_yticks(np.arange(len(list(pipe_XG[:-1].get_feature_names_out()))), labels=list(pipe_XG[:-1].get_feature_names_out()),fontsize=15)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(list(pipe_XG[:-1].get_feature_names_out()))):\n",
    "    for j in range((len(list(pipe_XG[:-1].get_feature_names_out()))-1),(i-1),-1):\n",
    "        if (SHAP_hypothesis_results.to_numpy()[i,j]<0.001):\n",
    "            text = ax.text(i, j,np.round(SHAP_hypothesis_results.to_numpy(),5)[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\",fontsize=15)\n",
    "        else:\n",
    "            text = ax.text(i, j,'NS',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\",fontsize=15)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810724cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation for shap values FP or TP\n",
    "np.std(shap_interaction_values_TP[:,1,1]) # choose your fature shap in dimensions - TP\n",
    "np.std(shap_interaction_values_FP[:,1,1]) # choose your fature shap in dimensions - FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine feature value (not shap) of FP and TP\n",
    "df_test.loc[(y_testtt==1) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)].describe()\n",
    "df_test.loc[(y_testtt==0) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)].describe()\n",
    "\n",
    "# perform univariate hypothesis testing betweeen FP and TP for each feature as described \n",
    "mannwhitneyu(df_test.loc[(y_testtt==1) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)]['ALT'],\n",
    "             df_test.loc[(y_testtt==0) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)]['ALT'],nan_policy='omit')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall plot with shap\n",
    "\n",
    "shap_values = explainer(xg_tune.best_estimator_[0:2].transform(df_test))\n",
    "shap.plots.waterfall(shap_values[12],max_display=15,show=False)# 12 false positive\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "\n",
    "ax.set_yticks(np.arange(0,15,1),labels=['TG (116) and ALP (114)','Female=Yes','LDL-c=130','Cholesterol=209','ALT=33','HDL-c=55','Hypertension=Yes','DM or IFG=No','Age=63','HbA1c=5.5','AST=31','Platletes=256','GGT=31','BMI=39','Albumin=3.6'],fontsize=25)\n",
    "\n",
    "\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd98f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the SHAP interaction of GGT and ALT\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "points=ax.scatter(x=df_test.loc[(y_testtt==0) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)].ALT,y=shap_interaction_values_FP[:,4,7],c=df_test.loc[(y_testtt==0) & (xg_tune.best_estimator_.predict_proba(df_test)[:,1]>0.14)].GGT,cmap='viridis',vmin=20,vmax=150)\n",
    "bar=fig.colorbar(points,ax=ax, pad=0.08, shrink=0.6, aspect=10)\n",
    "bar.ax.set_title(\"GGT (U/L)\",fontsize=15)\n",
    "bar.ax.tick_params(labelsize=15)\n",
    "ax.set_xlabel('ALT (U/L)',fontsize=15)\n",
    "ax.set_ylabel('SHAP interaction value',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfa735",
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost calibration on the validation sample\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "calibrated_xg = CalibratedClassifierCV(xg_tune.best_estimator_, cv=\"prefit\",method=\"sigmoid\")\n",
    "calibrated_xg.fit(df_test, y_testtt)\n",
    "disp = CalibrationDisplay.from_estimator(xg_tune.best_estimator_, df_test, y_testtt,name='XGBoost')\n",
    "\n",
    "plt.xlabel('Mean predicted probability by XGBoost')\n",
    "plt.ylabel('Fraction of patients with advanced fibrosis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
